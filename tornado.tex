%!TEX root=index.tex

\newacronym{ncdc}{NCDC}{National Climatic Data Center}
\newacronym{soi}{SOI}{Southern Oscillation Index}
\newacronym{tvs}{TVS}{National Tornado Vortex Signature}
\newacronym{mda}{MDA}{National Mesoscyclone Detection Algorithm}
\newacronym{nosql}{NoSQL}{Not only SQL}

\section{Case Study: Tornado}
One business opportunity, currently being pursued by the CSC Data Services group, is tornado prediction within the United States. (CNK - why?) A primary carrier of property and casualty insurance would potentially be very interested in knowing the probability of tornado occurrence in geographical areas in which they have a large customer base. There are two months in the year which kick off tornado season in the United States: the warm season starting in March, and the cold season starting in November. Having accurate predictions of weekly seasonal tornado counts, months in advance, would constitute a significant business advantage over competitors. There are two specific business cases for the property carrier to focus on:
\begin{itemize}
    \item an actuary who will use the historical year-to-year probability and future counts to create overlays based on parcel level policy and claims data on a 50km x 50km geographical grid
    \item an underwriter who will use actuarial information to create the appropriate terms and rates offering for their customers that maximizes revenues at the lowest risk level
\end{itemize}
An industry analyst, working with CSC, identified the actuary as the primary business target [unpublished interview, 2013]. In order to produce the specific analysis for the actuary, CSC employee Dr. Dan Walker and part-time employee Dr. Christopher Anderson have determined that the necessary data sets for tornado prediction should contain the following [unpublished interview, 2013]:
\begin{itemize}
    \item 1 MB of \gls{soi} historical archives \cite{bom}
    \item 100 MB of \gls{tvs} and \gls{mda} \cite{hdss}
    \item 1.5 TB of \gls{ncdc} historical tornado report \cite{ncdc}
\end{itemize}
All of this data is in the public domain and freely available to anyone that has the resources to store and analyze it. As mentioned previously,  CSC currently uses only the summary information provided by Giovanni in the \climatedge product. While Giovanni has the advantage of requiring no local computational resources, it represents the tip of the proverbial data iceberg and does not help in quantitative prediction necessary for a tornado solution. Dr. Anderson has speculated that there exist variables stored within the three dimensional \gls{merra} archives that can be utilized to further improve prediction. This additional data would be:
\begin{itemize}
    \item 2.5 TB of \gls{merra} 3D historical \cite{mdisc}
\end{itemize}
\subsection{Objective}
For tornado prediction, the specific analytic we'd produce is a 50km x50km (based on input data granularity) geo-spatial grid of predictions of the probability of tornado occurrences expressed as a Poisson distribution \cite{anderson}. This analytic would be computed twice a year in the months leading up to the tornado season in order to give the carriers enough time to react to the data and to take into account recent data. It's possible to represent the resulting Poisson distribution in a number of ways, either textually or graphically. While an interactive \gls{html} geographical map may best present the results for human consumption, the goal should be to get the information into a system capable of overlaying it with parcel and policy data so as to present a complete picture for decision making. CSC has a policy administration tool for insurance carriers known as POINT IN \cite{point_in}. This product is widely used, making it an ideal platform to ingest the results of our predictive analytics as serialized XML containing the Poisson probabilities for each geographical grid.\\

\subsection{Implementation}
While the entirety of the data sets listed above can vary in size from a few hundred gigabytes to a few terabytes when you include in the research and development efforts to reduce uncertainty, it does not fit the accepted big data model. In a guest blog post, Michael Stonebraker gives four detailed definitions to the term big data\index{big data}\cite{stonebraker}. He boils it down to the same basic concepts that most of the non-technical world uses: velocity, volume, and variety. Even with data volumes approaching several terabytes, tornado prediction does not conform to the accepted criteria for big data. While it is certainly possible to process this data using a \gls{nosql} data store such as Cassandra \cite{cassandra} or HBase \cite{Hbase}, it is not a requirement, or even necessary. An efficient distributed processing algorithm, such as Map Reduce \cite{mapreduce} will be able to easily handle data sets of this size and generate the data necessary to compute the Poisson distribution.



