%!TEX root=index.tex
\newacronym{ncdc}{NCDC}{National Climatic Data Center}
\newacronym{soi}{SOI}{Southern Oscillation Index}
\newacronym{tvs}{TVS}{National Tornado Vortex Signature}
\newacronym{mda}{MDA}{National Mesoscyclone Detection Algorithm}
\newacronym{nosql}{NoSQL}{Not only SQL}
\section{Case Study: Tornado}
One business opportunity, currently being pursued by the \textsc{CSC} Data Services group, is tornado prediction within the United States. (CNK - why?) A primary carrier of property and casualty insurance would potentially be very interested in knowing the probability of tornado occurrence in geographical areas in which they have a large customer base. There are two months in the year which kick off tornado season in the United States: the warm season starting in March, and the cold season starting in November. Having accurate predictions of weekly seasonal tornado counts, months in advance, would constitute a significant business advantage over competitors. There are two specific business cases for the property carrier to focus on:
\begin{itemize}
    \item an actuary who will use the historical year-to-year probability and future counts to create overlays based on parcel level policy and claims data on a 50km x 50km geographical grid
    \item an underwriter who will use actuarial information to create the appropriate terms and rates offering for their customers that maximizes revenues at the lowest risk level
\end{itemize}
An industry analyst, working with \textsc{CSC}, identified the actuary as the primary business target [unpublished interview, 2013]. In order to produce the specific analysis for the actuary, \textsc{CSC} employee Dr. Dan Walker and part-time employee Dr. Christopher Anderson have determined that the necessary data sets for tornado prediction should contain the following [unpublished interview, 2013]:
\begin{itemize}
    \item 1 MB of \gls{soi} historical archives \cite{bom}
    \item 100 MB of \gls{tvs} and \gls{mda} \cite{hdss}
    \item 1.5 TB of \gls{ncdc} historical tornado report \cite{ncdc}
\end{itemize}
All of this data is in the public domain and freely available to anyone that has the resources to store and analyze it. Dr. Anderson has speculated that there exist variables stored within the three dimensional \gls{merra} archives that can be utilized to further improve prediction. This additional data would be:
\begin{itemize}
    \item 2.5 TB of \gls{merra} 3D historical \cite{mdisc}
\end{itemize}
\subsection{Objective}
For tornado prediction, the specific analytic produced is a 50km x50km (based on input data granularity) geo-spatial grid of predictions of the probability of tornado occurrences expressed as a Poisson distribution \cite{anderson}. This analytic would be computed twice a year in the months leading up to the tornado season in order to give the carriers enough time to react to the data and to take into account recent data. It's possible to represent the resulting Poisson distribution in a number of ways, either textually or graphically. While an interactive \gls{html} geographical map may best present the results for human consumption, the goal should be to get the information into a system capable of overlaying it with parcel and policy data so as to present a complete picture for decision making. \textsc{CSC} has a policy administration tool for insurance carriers known as POINT IN \cite{point_in}. This product is widely used, making it an ideal platform to ingest the results of our predictive analytics as serialized XML containing the Poisson probabilities for each geographical grid.
\subsection{Implementation}
While the entirety of the data sets listed above can vary in size from a few hundred gigabytes to a few terabytes, when you include in the research and development efforts to reduce uncertainty, it does not fit the accepted big data model. In a guest blog post, Michael Stonebraker gives four detailed definitions to the term big data\index{big data}\cite{stonebraker}. He boils it down to the same basic concepts that most of the non-technical world uses: velocity, volume, and variety. Even with data volumes approaching several terabytes for the research and development, tornado prediction does not conform to the accepted criteria for big data. 
\subsubsection{Compute}
Running the Poisson distribution generator across every grid cell will be computationally expensive. One way to offset this is parallel execution across a small compute cluster in the range of twelve to twenty four cores. If the optional \gls{merra} follow on work is pursued, temporary additional processors may be required to handle processing of the raw data.
\subsubsection{Storage}
While it is certainly possible to process this data using a \gls{nosql} data store such as Cassandra \cite{cassandra} or HBase \cite{Hbase}, it is not a requirement, or even necessary. With storage prices approaching several terabytes per \$100, a low end multi-core desktop computer, combined with an efficient distributed processing algorithm, such as Map Reduce \cite{mapreduce} will be able to easily handle data sets of this size and generate the data necessary to compute the Poisson distribution in a timely manner.\\

Table 1 lists a generic key-value model for storing any piece of climate data.
\begin{table}[htbp]
	\caption{Climate Data Model}
	\centering
	\begin{tabular}{l l}
		\hline
		Key & Value \\ [0.5ex]
		%heading
		\hline
		meta-data & what the measurement represents, i.e., total precipitation or soil moisture\\
		time & time and date of measurement in UTC, i.e., YYYY-MM-DD HH:MM:SS\\
		coordinates & geographic location in latitude and longitude, i.e. -90.0 to 90.0 and -180.0 to 180.0\\
		value & measurement value, i.e. 0.000052977\\
		\hline
	\end{tabular}
\end{table}
With this simple structure and specific materialized views \index{materialized views}, we enable analytics based on any combination of the following searches: temporal, geospatial, or by meta-data \cite{materialized_views}. 