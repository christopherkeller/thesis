%!TEX root=index.tex
\newacronym{ncdc}{NCDC}{National Climatic Data Center}
\newacronym{soi}{SOI}{Southern Oscillation Index}
\newacronym{tvs}{TVS}{National Tornado Vortex Signature}
\newacronym{nosql}{NoSQL}{Not only SQL}
\newacronym{xml}{XML}{Extensible Markup Language}

\section{Use Cases}
[ paragraph that explores these]
Two offerings with sufficiently different data and processing requirements are tornado and flood. While both are similar in  respect that the objective is to produce a probability of occurrence for any given geographical point in the coverage area, the technology required to accomplish this varies greatly. A primary carrier of property and casualty insurance would be very interested in knowing the probability of natural catastrophes in geographical areas in which they have a large customer base. Having accurate predictions of seasonal event probabilities, months in advance, constitutes a significant business advantage over competitors when issuing or renewing premiums. Insurance underwriters will use event probabilities to create the appropriate terms and rates offerings for their customers that maximize revenues at the lowest risk level. [CNK expand Novarica Report, 2013]. [ more advanced discussion of the business model need ]
    [ how are they going to use our data to make better business decisions] \\
\subsection{Offering}
Quantitative predictions for natural catastrophic events such as tornado, hail, or flood can be expressed as a Poisson distribution showing the probability of a given number of events occurring over a fixed interval of time \cite{anderson}. It is possible to represent the resulting Poisson distribution in a number of ways, either textually or graphically. While an interactive \gls{html} geographical map may best present the results visually, the goal is to get the information into a system capable of overlaying it with parcel and policy data so as to present a complete picture for decision making. \textsc{CSC} has two different administration tools for insurance carriers called POINT IN \cite{point_in} and Exceed \cite{exceed} which can handles serialized \gls{xml} input feeds containing the tornado probabilities for each geographical grid location. For the global flood offering, the Integral software package from \textsc{CSC} will serve the same purpose \cite{integral}.
\subsection{Big Data}
In a guest blog post, Michael Stonebraker gives four detailed definitions to the term big data\index{big data}\cite{stonebraker}.  Big data is classified as having one of the following attributes: velocity, volume, or variety.  Velocity describes the frequency at which data is expected to arrive, either pushed or pulled. Volume is simply the size of the data sets to be ingested, stored, and processed. Variety describes the various heterogeneous sources and formats which comprise the data sets. A big data label can therefore be applied to a data set having  one or more of these attributes.\\

The initial input data sets for tornado are the following \cite{walker}:
\begin{itemize}
    \item 1 MB of \gls{soi} historical archives \cite{bom}
    \item 100 MB of \gls{tvs} \cite{hdss}
    \item 1.5 TB of \gls{ncdc} historical tornado report \cite{ncdc}
\end{itemize}
All of this data is in the public domain and freely available to anyone that has the resources to store and analyze it. Dr. Christopher Anderson has speculated that there exist variables stored within the three dimensional \gls{merra} archives that can be utilized to further improve prediction. This additional data would be:
\begin{itemize}
    \item 1.5 TB of \gls{merra} 3D historical \cite{mdisc}
\end{itemize}
With data volumes approaching several terabytes for the full effort, tornado prediction does not conform to the criteria for big data because it lacks all three attributes. The incoming data velocity can be based on several factors: how often is the input data available, how frequently is the output generated, and how long it takes to processes the incoming data. The tornado analytic is expected to be generated monthly which means input must be processed at least that often. The \gls{soi} and \gls{tvs} data sets are small enough to be considered noise with respect to the larger \gls{merra} data.  Even at several terabytes, \gls{merra} can still fit on one or two hard drives, eliminating the need for more than one system to store all the data.  Four data sets will require four different parsers to process and ingest the data. Preliminary tests have shown that processing a single day of \gls{merra} (300MB) and writing into an SQLite flat file takes approximately 20 seconds per variable a modern CPU \cite{keller1}.\\

The flood analytics requires the following initial public domain data sets \cite{walker}:
\begin{itemize}
    \item \gls{nasa} satellite rainfall data
    \item 2.5 TB \gls{merra} soil moisture, atmospheric wind, humidity, and runoff data
    \item streamflow data as available
    \item damage data from previous floods (newspaper, social media, etc) to augment streamflow
    \item satellite data of built structures and inundation
    \item hydrologic response model [CNK]
\end{itemize}
Recalling the attributes of big data: velocity, volume, and variety, it becomes clear that flood prediction falls into this category more so than tornado data. It is expected that ingesting relevant newspaper and social media data will add a daily component with reports of damage, especially during the various flood seasons. Even without taking into account the global nature of the offering for flood prediction, the satellite rainfall and \gls{merra} data sets will be larger than the corresponding sets in tornado (mainly due to more input variables). Without even addressing existing structures, the entirety of the input data set is larger. Much of the data sets are computer published, leading to structured input, however the media component implies unstructured data. \\

The \gls{soi} data set is primarily used as an indicator of an El Niño or La Niña trend which is useful in predictive climate analytics. This data set is updated monthly by the Australian Bureau of Meteorology, giving us a monthly baseline for how frequently our \climatedge offerings are produced. While specific customers may or may not require monthly feeds, the methods will be in place to generate them at that frequency.\\

As seen in table \ref{qualifiers},  the flood analytic ingests a larger and wider variety of data sets at a faster pace than the corresponding tornado analytic. Although the format and frequency of both analytic offerings are similar, the infrastructure needed to produce those offerings are quite different in scale.\\
\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l}
        \hline
        Attribute & Tornado & Flood\\ [0.5ex]
        \hline
        data velocity & monthly & daily\\
        data volume &  3TB+  & 1 PB+\\
        data variety &  4 sets all structured & 6 sets some structured\\
        \hline
    \end{tabular}
    \caption{Big Data Qualifiers}
    \label{qualifiers}
\end{table}
\subsection{Velocity, Volume, and Variety}
The speed at which sets are processed and stored vary with respect to the particular data. Some data sets are generated daily, e.g. \gls{merra}, some are generated monthly, e.g. \gls{soi}, and some require daily parsing, e.g. newspapers, Twitter \index{Twitter}, or other social media. Although the velocity of incoming data varies by type, it is still expected the output analytics for both tornado and flood are generated monthly on a fixed schedule as determined by the specific customer. If a months worth of data can be be processed in a timely manner, there is no reason to move to daily feeds. Tests were performed on one month of  \gls{merra} (representative of a larger data feed) and shown that on a single 3GHz core, it takes approximately thirty five minutes to parse and store the data, exclusive of retrieval time. It is not expected that monthly retrieval of the structured data sets will present a problem. The unstructured will need to be gathered  on a daily or semi-daily basis in order to stay current. If not retrieved in a timely manner, there is risk of the data disappearing or time spent processing weekly or monthly bulk loads is prohibitive.\\

It is possible to store  all the necessary data required for the tornado analytic on a 1U server with one terabyte drives. Many home computers fall into this category. In contrast, if we stored four drives of four terabytes each in 1U server, we would require sixty four servers to accommodate one petabyte worth of flood data. Replicating the data for high availability adds a factor of two or three to those numbers. Clearly, flood data requires capacity that is orders of magnitude greater than that of the tornado data. An important point is that the persistent storage of data after pre-processing could be significantly less what is needed to process it. For example, \gls{merra} files are typically 100 to 300 MB in size, depending on whether it's two dimensional or three dimensional. These data files contain variables that may or may not be useful in specific offerings. It is possible that out of over thirty variables, only two or three would actually be used which dramatically reduces the amount of persistent data stored and indexed. The data parsers would know which data they need to permanently store and discard the rest. For this reason, it is crucial that the storage used for pre-processing be elastic to save cost.\\

Between the tornado and flood data sets, there are nine separate data sets that need to be pre-processed and stored. While the technology and approach to extracting and storing relevant information from each data is similar, they are still distinct processes that must be developed individually. All four data sets for the tornado analytic are structured with well defined approaches for extracting the required data.  A \gls{merra} parser demonstrating variable extraction and storage has been included in the appendix.  The flood analytic also contains several data sets that are well structured and do not present any significant challenge to data extraction on a continual basis. Within the flood analytic is the unstructured newspaper and associated social media data related to flood reports and damage. This will require a significantly different parsing technology than the structured data and will be the most challenging to develop, implement, and refine.\\

The flood analytic requires data sets with different attributes than that of tornado. The real time nature of newspaper articles and other social media requires daily or semi-daily ingestion. Significant flooding in a key geographic region may require an out of band delivery of the associated analytics.  This information will be reported first by the media near real time and not seen in the structured feeds until the end of the month. The additional data sets required by the global and domestic flood offering surpass that of tornado by many times. The same storage technology, but not necessarily the same platform, will work for both offerings at different scales. Tornado data is well structured, while flood data is both structured and unstructured. Different algorithms, involving semantic and natural language processing, are necessary to effectively parse the variety of feeds associated with flood reporting in the media.  The attributes shown by the flood  data sets map very well to the accepted criteria of big data, while tornado does not. The technology necessary to produce accurate flood analytics are more complex than tornado and at scale.  The expertise developed by building a platform framework capable of generating analytics on both structured and unstructured data will naturally transfer to the next \climatedge offering.