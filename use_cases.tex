%!TEX root=index.tex
\newacronym{ncdc}{NCDC}{National Climatic Data Center}
\newacronym{soi}{SOI}{Southern Oscillation Index}
\newacronym{tvs}{TVS}{National Tornado Vortex Signature}
\newacronym{nosql}{NoSQL}{Not only SQL}
\section{Offerings}
[ paragraph that explores these]

\subsection{Tornado}
One business opportunity, currently being pursued by the \textsc{CSC} Data Services group, is tornado prediction within the United States. (CNK - why?) 

[ the why is in the weather congress docs that dan shared]

A primary carrier of property and casualty insurance would potentially be very interested in knowing the probability of tornado occurrence in geographical areas in which they have a large customer base. There are two months in the year which kick off tornado season in the United States: the warm season starting in March, and the cold season starting in November. Having accurate predictions of weekly seasonal tornado counts, months in advance, would constitute a significant business advantage over competitors. There are two specific business cases for the insurance carrier to focus on:
\begin{itemize}
    \item an actuary who will use the historical year-to-year probability and future counts to create overlays based on parcel level policy and claims data on a 50km x 50km geographical grid
    \item an underwriter who will use actuarial information to create the appropriate terms and rates offering for their customers that maximizes revenues at the lowest risk level
    \item CNK we need to rewrite these bullets from the challenger deck
\end{itemize}
An industry analyst, working with \textsc{CSC}, identified the chief underwriter as the primary business target [ CNK expand Noverica Report, 2013]. \\

In order to produce the specific analysis for the actuary, \textsc{CSC} employee Dr. Dan Walker and part-time employee Dr. Christopher Anderson have determined that the necessary data sets for tornado prediction should contain the following [unpublished interview, 2013]:
\begin{itemize}
    \item 1 MB of \gls{soi} historical archives \cite{bom}
    \item 100 MB of \gls{tvs} \cite{hdss}
    \item 1.5 TB of \gls{ncdc} historical tornado report \cite{ncdc}
\end{itemize}
All of this data is in the public domain and freely available to anyone that has the resources to store and analyze it. Dr. Anderson has speculated that there exist variables stored within the three dimensional \gls{merra} archives that can be utilized to further improve prediction. This additional data would be:
\begin{itemize}
    \item 2.5 TB of \gls{merra} 3D historical \cite{mdisc}
\end{itemize}
\subsection{Objective}

[ more advanced discussion of the business model need ]
[ how are they going to use our data to make better business decisions]

For tornado prediction, the specific analytic produced is a 50km x50km (based on input data granularity) geo-spatial grid of predictions of the probability of tornado occurrences expressed as a Poisson distribution \cite{anderson}. This analytic would be computed monthly [for two tornado seasons] in order to give the carriers enough time to react to the data and to take into account recent data. It's possible to represent the resulting Poisson distribution in a number of ways, either textually or graphically. While an interactive \gls{html} geographical map may best present the results for human consumption, the goal should be to get the information into a system capable of overlaying it with parcel and policy data so as to present a complete picture for decision making. \textsc{CSC} has a policy administration tool for insurance carriers known as POINT IN \cite{point_in}.

[exceed is a another tornado tool]

 This product is widely used, making it an ideal platform to ingest the results of our predictive analytics as serialized XML containing the Poisson probabilities for each geographical grid.
\subsubsection{Implementation}
While the entirety of the data sets listed above can vary in size from a few hundred gigabytes to a few terabytes, when include in the research and development efforts to reduce uncertainty, it does not fit the accepted big data model. [rewrite]



In a guest blog post, Michael Stonebraker gives four detailed definitions to the term big data\index{big data}\cite{stonebraker}. 

[ get rid of the word he]
He boils it down to the same basic concepts that most of the non-technical world uses: velocity, volume, and variety. Even with data volumes approaching several terabytes for the research and development, tornado prediction does not conform to the accepted criteria for big data because it lacks ....

[ what are the critical capabilities that we need to deliver]

\subsubsection{Compute}
Running the Poisson distribution generator across every grid cell will be computationally expensive. One way to offset this is parallel execution across a small compute cluster in the range of twelve to twenty four cores. If the optional \gls{merra} follow on work is pursued, temporary additional processors may be required to handle processing of the raw data.
\subsubsection{Storage}
While it is certainly possible to process this data using a \gls{nosql} data store such as Cassandra \cite{cassandra} or HBase \cite{Hbase}, it is not a requirement, or even necessary. With storage prices approaching several terabytes per \$100, a low end multi-core desktop computer, combined with an efficient distributed processing algorithm, such as MapReduce \cite{mapreduce} will be able to easily handle data sets of this size and generate the intermediate results necessary to compute the Poisson distribution in a timely manner.\\

Table 1 lists a generic key-value model for storing any piece of climate data.
\begin{table}[htbp]
	\caption{Climate Data Model}
	\centering
	\begin{tabular}{l l}
		\hline
		Key & Value \\ [0.5ex]
		%heading
		\hline
		meta-data & what the measurement represents, e.g., total precipitation or soil moisture\\
		time & time and date of measurement in UTC, e.g., YYYY-MM-DD HH:MM:SS\\
		coordinates & geographic location in latitude and longitude, e.g. -90.0 to 90.0 and -180.0 to 180.0\\
		value & measurement value, e.g. 0.000052977\\
		\hline
	\end{tabular}
\end{table}
With this simple structure and specific materialized views \index{materialized views},  analytics are enabled based on any combination of the following searches: temporal, geospatial, or by meta-data \cite{materialized_views}. 
%!TEX root=index.tex
\subsection{Flood}
A second business opportunity, both global and domestic flood insurance, is currently in the early planning stages within the CSC Data Services group due to its similarity to the tornado use case, making it an ideal next step for CSC. 

[rewrite first sentence into more than one sentence]


 As before, the  insurance underwriter is the preferred target:
\begin{itemize}
    \item an actuary who will use the historical year-to-year probability and future counts to create overlays based on parcel level policy and claims data
    \item an underwriter who will use actuarial information to create the appropriate terms and rates offering for their customers that maximizes revenues at the lowest risk level
\end{itemize}
The flood related data is less defined, but far more complex, than the tornado data. 

[ better way to cite]
Again, Dr. Christopher Anderson and Dr. Dan Walker have determined that the necessary data sets to be [unpublished interview, 2013]:
\begin{itemize}
    \item NASA satellite rainfall data
    \item \gls{merra} soil moisture, atmospheric wind,  humidity, and runoff data
    \item streamflow data as available
    \item damage data from previous floods (newspaper, social media, etc) to augment streamflow
    \item satellite data of built structures and inundation
    \item hydrologic response model [CNK]
\end{itemize}
As with the tornado data, most if not all, is in the public domain and freely available.
\subsubsection{Objective}
The year-to-year probability of the number of floods can be expressed in manner similar tornados, using a Poisson distribution. The analytics would be computed several times per year in order to account for new data and provided either on demand via a portal, an XML feed to POINT IN, or via customized  report, similar to \climatedge.

[integral is a tool for flood]


\subsubsection{Implementation}
Recalling the attributes of big data: velocity, volume, and variety, it becomes clear that flood prediction falls into this category more so than tornado data. It is expected that ingesting relevant newspaper and social media data will add a real time component, especially during the various flood seasons. Even without taking into account the global nature of the offering for flood prediction, the satellite rainfall and \gls{merra} data sets will be larger than the corresponding sets in tornado (mainly due to more input variables). Without even addressing existing structures, the entirety of the input data set is larger. Much of the data sets are computer published, leading to well formed input, however the media component implies significant variety in the data.  This offering would benefit from a big data platform to handle everything from storing and indexing to analytics.
\subsubsection{Compute}
 With vastly more raw data used as input for the analytics,
 [what are we trying to stay with the word vastly more]
 
  a larger dedicated group of pre-processing cores will be required. As with tornado, running the Poisson distribution generator across every grid cell will be computationally expensive, so twenty four to forty eight cores would be a good initial estimate. Parallel execution is again preferred, but in the twenty four to forty eight core range. 
\subsubsection{Storage}
With larger and more diverse data sets comes the need for more storage. The work on flood prediction was not as advanced as tornado, so accurate estimates on data sizes was not available. Best guesses put the storage requirements in the ballpark of hundreds of terabytes to petabytes, requiring a substantially greater infrastructure investment than tornado.