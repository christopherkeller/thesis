%!TEX root=index.tex
\section{Case Study: Flood}
A second business opportunity, both global and domestic flood insurance, is currently in the early planning stages within the CSC Data Services group due to its similarity to the tornado use case, making it an ideal next step for CSC. 

[rewrite first sentence into more than one sentence]


 As before, the  insurance underwriter is the preferred target:
\begin{itemize}
    \item an actuary who will use the historical year-to-year probability and future counts to create overlays based on parcel level policy and claims data
    \item an underwriter who will use actuarial information to create the appropriate terms and rates offering for their customers that maximizes revenues at the lowest risk level
\end{itemize}
The flood related data is less defined, but far more complex, than the tornado data. 

[ better way to cite]
Again, Dr. Christopher Anderson and Dr. Dan Walker have determined that the necessary data sets to be [unpublished interview, 2013]:
\begin{itemize}
    \item NASA satellite rainfall data
    \item \gls{merra} soil moisture, atmospheric wind,  humidity, and runoff data
    \item streamflow data as available
    \item damage data from previous floods (newspaper, social media, etc) to augment streamflow
    \item satellite data of built structures and inundation
    \item hydrologic response model [CNK]
\end{itemize}
As with the tornado data, most if not all, is in the public domain and freely available.
\subsection{Objective}
The year-to-year probability of the number of floods can be expressed in manner similar tornados, using a Poisson distribution. The analytics would be computed several times per year in order to account for new data and provided either on demand via a portal, an XML feed to POINT IN, or via customized  report, similar to ClimatEdge\texttrademark{}.

[integral is a tool for flood]


\subsection{Implementation}
Recalling the attributes of big data: velocity, volume, and variety, it becomes clear that flood prediction falls into this category more so than tornado data. It is expected that ingesting relevant newspaper and social media data will add a real time component, especially during the various flood seasons. Even without taking into account the global nature of the offering for flood prediction, the satellite rainfall and \gls{merra} data sets will be larger than the corresponding sets in tornado (mainly due to more input variables). Without even addressing existing structures, the entirety of the input data set is larger. Much of the data sets are computer published, leading to well formed input, however the media component implies significant variety in the data.  This offering would benefit from a big data platform to handle everything from storing and indexing to analytics.
\subsubsection{Compute}
 With vastly more raw data used as input for the analytics,
 [what are we trying to stay with the word vastly more]
 
  a larger dedicated group of pre-processing cores will be required. As with tornado, running the Poisson distribution generator across every grid cell will be computationally expensive, so twenty four to forty eight cores would be a good initial estimate. Parallel execution is again preferred, but in the twenty four to forty eight core range. 
\subsubsection{Storage}
With larger and more diverse data sets comes the need for more storage. The work on flood prediction was not as advanced as tornado, so accurate estimates on data sizes was not available. Best guesses put the storage requirements in the ballpark of hundreds of terabytes to petabytes, requiring a substantially greater infrastructure investment than tornado.


