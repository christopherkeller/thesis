%!TEX root=index.tex
\newacronym{api}{API}{Application Programming Interface}
\newacronym{it}{IT}{Information Technology}
\newacronym{tcpip}{TCP/IP}{Transmission Control Protocol Internet Protocol}
\section{Future Directions}
One attribute of any big data problem is that of volume. Some offerings have larger volume, some do not, but every big data solution  has to provide data access that meets the offering goals, including cost effectiveness. With respect to \climatedge, petabytes of publicly accessible climate data reside at the various  government agencies which collected them. There are several broad approaches to the problem of remote data access:
\begin{itemize}
    \item provide an \gls{api} that facilitates access the data at its remote location
    \item clone the data and store it locally within a \textsc{CSC} data center
    \item process the data remotely on site
\end{itemize}
This section takes a look at each of the three alternatives and the advantageous and disadvantageous of each approach.
\subsection{\gls{api}}
Many federal data sets are already publicly available in compressed formats, usually by time or type. In order to provide data as a service, the provider would need implement various \gls{api}s allowing the consumer to slice and dice the data sets as necessary and on demand. This has the advantage of being very simple to implement on the part of the consumer. Simply issue the request, retrieve the results, and implement the analytics locally. There are two obvious problems with this approach: latency of the requests rules out real time analytics and the enormous network bandwidth requirements of everyone pulling down data simultaneously. Local storage space is still required, at least as large as the largest possible retrieval, making this a questionable approach from the start. Additionally, the data provider needs to provide an \gls{api} capable of the granularity of individual results as well as the breath of the entire data set. This type of approach simply does not scale well to the largest data sets and places a burden on strained federal agency \gls{it} budgets.
\subsection{Cloning}
Possibly the most common approach is to clone the data to a local data center. The benefits are obvious: network adjacent access speeds, self imposed reliability, and freedom to store and index the data in any way necessary.  Even the initial population of historical data sets can be managed by project timelines or even by shipping tapes or hard disks between data centers. The downside, of course, is the immense cost in replicating peta scale data sets locally. Even with storage costs dropping and data center efficiencies increasing, the outlay of capital necessary to stand up a large data center is significant. One often overlooked issue with duplicating public data sets is in quality assurance and quality control. It is trivial it ensure exact duplicates of data have been created, \gls{tcpip} and checksumming are longstanding solutions. What happens if an error is discovered in the original copy of the data? What if that error leads to erroneous results that are passed onto customers?  While the U.S. government does not always prevail in weather related liability proceedings, most of the time they win on the basis of immunity  \cite[Chapter~4]{fairweather}. 
\subsection{Appliances}
A hybrid approach of first two cases may lie in processing the data locally at each customer site. At least one government agency is developing an analytics cluster to work with customers and partners on processing large data sets on site \cite{duffy}. While this approach is heavily dependent on cooperation (possibly funding) from the various agencies, it is certainly worth investigation. The benefits lie in possible reduced storage costs and reduced network costs since only summary results are transferred. It is reasonable to assume that different companies would have different requirements for analyzing the same data sets. These differences lead to multiple methods of storing and indexing the data efficiently, which would be prohibitively expensive for agencies to undertake.  One solution may lie in the development of a \textsc{CSC} big data appliance for remote storage and analytics. This appliance could be dropped into the agency data centers and, through secure \gls{api}s, handle the analytics on site, thus saving in network transfer costs. The proposed framework in this paper could easily be transformed into an appliance form factor. \textsc{CSC} is currently exploring technologies such as Cetas for its own solutions, \index{Cetas} which include many of the components necessary for an appliance \cite{cetas}.\\

Within these three broad categories exist other hybrids such as federal data centers whose mandate is to provide public data access. As with any problem, there are multiple solutions possible.  Three approaches were proposed with various advantages and disadvantages. Any one, or combination, would be suitable for a thesis centered around methods of processing large data sets. As big data technology evolves, and data sets grow increasingly larger, methods of efficiently analyzing publicly available data will be forefront in discussions.